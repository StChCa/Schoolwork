{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Five Assignment: Cartpole Problem\n",
    "Review the code in this notebook and in the score_logger.py file in the *scores* folder (directory). Once you have reviewed the code, return to this notebook and select **Cell** and then **Run All** from the menu bar to run this code. The code takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            if step > 100:\n",
    "                return\n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 11\n",
      "Scores: (min: 11, avg: 11, max: 11)\n",
      "\n",
      "Run: 2, exploration: 0.8911090557802088, score: 32\n",
      "Scores: (min: 11, avg: 21.5, max: 32)\n",
      "\n",
      "Run: 3, exploration: 0.7940753492934954, score: 24\n",
      "Scores: (min: 11, avg: 22.333333333333332, max: 32)\n",
      "\n",
      "Run: 4, exploration: 0.6498078359349755, score: 41\n",
      "Scores: (min: 11, avg: 27, max: 41)\n",
      "\n",
      "Run: 5, exploration: 0.6057704364907278, score: 15\n",
      "Scores: (min: 11, avg: 24.6, max: 41)\n",
      "\n",
      "Run: 6, exploration: 0.5618938591163328, score: 16\n",
      "Scores: (min: 11, avg: 23.166666666666668, max: 41)\n",
      "\n",
      "Run: 7, exploration: 0.46211964903917074, score: 40\n",
      "Scores: (min: 11, avg: 25.571428571428573, max: 41)\n",
      "\n",
      "Run: 8, exploration: 0.42650460709830135, score: 17\n",
      "Scores: (min: 11, avg: 24.5, max: 41)\n",
      "\n",
      "Run: 9, exploration: 0.40769130904675194, score: 10\n",
      "Scores: (min: 10, avg: 22.88888888888889, max: 41)\n",
      "\n",
      "Run: 10, exploration: 0.3858205374665315, score: 12\n",
      "Scores: (min: 10, avg: 21.8, max: 41)\n",
      "\n",
      "Run: 11, exploration: 0.37251769488706843, score: 8\n",
      "Scores: (min: 8, avg: 20.545454545454547, max: 41)\n",
      "\n",
      "Run: 12, exploration: 0.35608578229633, score: 10\n",
      "Scores: (min: 8, avg: 19.666666666666668, max: 41)\n",
      "\n",
      "Run: 13, exploration: 0.3403786882983606, score: 10\n",
      "Scores: (min: 8, avg: 18.923076923076923, max: 41)\n",
      "\n",
      "Run: 14, exploration: 0.32864265128599696, score: 8\n",
      "Scores: (min: 8, avg: 18.142857142857142, max: 41)\n",
      "\n",
      "Run: 15, exploration: 0.30945741577570285, score: 13\n",
      "Scores: (min: 8, avg: 17.8, max: 41)\n",
      "\n",
      "Run: 16, exploration: 0.29729358661854943, score: 9\n",
      "Scores: (min: 8, avg: 17.25, max: 41)\n",
      "\n",
      "Run: 17, exploration: 0.2827589419554058, score: 11\n",
      "Scores: (min: 8, avg: 16.88235294117647, max: 41)\n",
      "\n",
      "Run: 18, exploration: 0.27164454854530906, score: 9\n",
      "Scores: (min: 8, avg: 16.444444444444443, max: 41)\n",
      "\n",
      "Run: 19, exploration: 0.2583638820072446, score: 11\n",
      "Scores: (min: 8, avg: 16.157894736842106, max: 41)\n",
      "\n",
      "Run: 20, exploration: 0.24450384299593592, score: 12\n",
      "Scores: (min: 8, avg: 15.95, max: 41)\n",
      "\n",
      "Run: 21, exploration: 0.23371867538818816, score: 10\n",
      "Scores: (min: 8, avg: 15.666666666666666, max: 41)\n",
      "\n",
      "Run: 22, exploration: 0.22229219984074702, score: 11\n",
      "Scores: (min: 8, avg: 15.454545454545455, max: 41)\n",
      "\n",
      "Run: 23, exploration: 0.21036724137609603, score: 12\n",
      "Scores: (min: 8, avg: 15.304347826086957, max: 41)\n",
      "\n",
      "Run: 24, exploration: 0.19709615934585656, score: 14\n",
      "Scores: (min: 8, avg: 15.25, max: 41)\n",
      "\n",
      "Run: 25, exploration: 0.1903004112552766, score: 8\n",
      "Scores: (min: 8, avg: 14.96, max: 41)\n",
      "\n",
      "Run: 26, exploration: 0.18190617987607657, score: 10\n",
      "Scores: (min: 8, avg: 14.76923076923077, max: 41)\n",
      "\n",
      "Run: 27, exploration: 0.17475600159032884, score: 9\n",
      "Scores: (min: 8, avg: 14.555555555555555, max: 41)\n",
      "\n",
      "Run: 28, exploration: 0.16373146555890544, score: 14\n",
      "Scores: (min: 8, avg: 14.535714285714286, max: 41)\n",
      "\n",
      "Run: 29, exploration: 0.15650920157696743, score: 10\n",
      "Scores: (min: 8, avg: 14.379310344827585, max: 41)\n",
      "\n",
      "Run: 30, exploration: 0.15035730121053842, score: 9\n",
      "Scores: (min: 8, avg: 14.2, max: 41)\n",
      "\n",
      "Run: 31, exploration: 0.14444721332374086, score: 9\n",
      "Scores: (min: 8, avg: 14.03225806451613, max: 41)\n",
      "\n",
      "Run: 32, exploration: 0.13669828187021155, score: 12\n",
      "Scores: (min: 8, avg: 13.96875, max: 41)\n",
      "\n",
      "Run: 33, exploration: 0.13001512070402377, score: 11\n",
      "Scores: (min: 8, avg: 13.878787878787879, max: 41)\n",
      "\n",
      "Run: 34, exploration: 0.1242800989098765, score: 10\n",
      "Scores: (min: 8, avg: 13.764705882352942, max: 41)\n",
      "\n",
      "Run: 35, exploration: 0.11939502647758558, score: 9\n",
      "Scores: (min: 8, avg: 13.628571428571428, max: 41)\n",
      "\n",
      "Run: 36, exploration: 0.11412846151764894, score: 10\n",
      "Scores: (min: 8, avg: 13.527777777777779, max: 41)\n",
      "\n",
      "Run: 37, exploration: 0.1085487359239089, score: 11\n",
      "Scores: (min: 8, avg: 13.45945945945946, max: 41)\n",
      "\n",
      "Run: 38, exploration: 0.10170090558064004, score: 14\n",
      "Scores: (min: 8, avg: 13.473684210526315, max: 41)\n",
      "\n",
      "Run: 39, exploration: 0.09770335251664321, score: 9\n",
      "Scores: (min: 8, avg: 13.35897435897436, max: 41)\n",
      "\n",
      "Run: 40, exploration: 0.09292664835904782, score: 11\n",
      "Scores: (min: 8, avg: 13.3, max: 41)\n",
      "\n",
      "Run: 41, exploration: 0.08972259762946533, score: 8\n",
      "Scores: (min: 8, avg: 13.170731707317072, max: 41)\n",
      "\n",
      "Run: 42, exploration: 0.08576489601717459, score: 10\n",
      "Scores: (min: 8, avg: 13.095238095238095, max: 41)\n",
      "\n",
      "Run: 43, exploration: 0.08075818212241151, score: 13\n",
      "Scores: (min: 8, avg: 13.093023255813954, max: 41)\n",
      "\n",
      "Run: 44, exploration: 0.07719590465791494, score: 10\n",
      "Scores: (min: 8, avg: 13.022727272727273, max: 41)\n",
      "\n",
      "Run: 45, exploration: 0.07196434741762824, score: 15\n",
      "Scores: (min: 8, avg: 13.066666666666666, max: 41)\n",
      "\n",
      "Run: 46, exploration: 0.06810378976195819, score: 12\n",
      "Scores: (min: 8, avg: 13.043478260869565, max: 41)\n",
      "\n",
      "Run: 47, exploration: 0.06477420436570952, score: 11\n",
      "Scores: (min: 8, avg: 13, max: 41)\n",
      "\n",
      "Run: 48, exploration: 0.05978212923978115, score: 17\n",
      "Scores: (min: 8, avg: 13.083333333333334, max: 41)\n",
      "\n",
      "Run: 49, exploration: 0.05714511431233153, score: 10\n",
      "Scores: (min: 8, avg: 13.020408163265307, max: 41)\n",
      "\n",
      "Run: 50, exploration: 0.05407954064343768, score: 12\n",
      "Scores: (min: 8, avg: 13, max: 41)\n",
      "\n",
      "Run: 51, exploration: 0.050162503748802344, score: 16\n",
      "Scores: (min: 8, avg: 13.058823529411764, max: 41)\n",
      "\n",
      "Run: 52, exploration: 0.04771006548501318, score: 11\n",
      "Scores: (min: 8, avg: 13.01923076923077, max: 41)\n",
      "\n",
      "Run: 53, exploration: 0.04583472801998072, score: 9\n",
      "Scores: (min: 8, avg: 12.943396226415095, max: 41)\n",
      "\n",
      "Run: 54, exploration: 0.04403310436296763, score: 9\n",
      "Scores: (min: 8, avg: 12.87037037037037, max: 41)\n",
      "\n",
      "Run: 55, exploration: 0.04230229704853423, score: 9\n",
      "Scores: (min: 8, avg: 12.8, max: 41)\n",
      "\n",
      "Run: 56, exploration: 0.04063952250178857, score: 9\n",
      "Scores: (min: 8, avg: 12.732142857142858, max: 41)\n",
      "\n",
      "Run: 57, exploration: 0.03694767635022522, score: 20\n",
      "Scores: (min: 8, avg: 12.859649122807017, max: 41)\n",
      "\n",
      "Run: 58, exploration: 0.03461682084029365, score: 14\n",
      "Scores: (min: 8, avg: 12.879310344827585, max: 41)\n",
      "\n",
      "Run: 59, exploration: 0.031948941366546975, score: 17\n",
      "Scores: (min: 8, avg: 12.94915254237288, max: 41)\n",
      "\n",
      "Run: 60, exploration: 0.030693125720441184, score: 9\n",
      "Scores: (min: 8, avg: 12.883333333333333, max: 41)\n",
      "\n",
      "Run: 61, exploration: 0.02948667236521416, score: 9\n",
      "Scores: (min: 8, avg: 12.819672131147541, max: 41)\n",
      "\n",
      "Run: 62, exploration: 0.02832764101944931, score: 9\n",
      "Scores: (min: 8, avg: 12.758064516129032, max: 41)\n",
      "\n",
      "Run: 63, exploration: 0.02707809682994571, score: 10\n",
      "Scores: (min: 8, avg: 12.714285714285714, max: 41)\n",
      "\n",
      "Run: 64, exploration: 0.025883670561501974, score: 10\n",
      "Scores: (min: 8, avg: 12.671875, max: 41)\n",
      "\n",
      "Run: 65, exploration: 0.022720922640519125, score: 27\n",
      "Scores: (min: 8, avg: 12.892307692307693, max: 41)\n",
      "\n",
      "Run: 66, exploration: 0.021827832324362372, score: 9\n",
      "Scores: (min: 8, avg: 12.833333333333334, max: 41)\n",
      "\n",
      "Run: 67, exploration: 0.020348562734319765, score: 15\n",
      "Scores: (min: 8, avg: 12.865671641791044, max: 41)\n",
      "\n",
      "Run: 68, exploration: 0.01954872266561937, score: 9\n",
      "Scores: (min: 8, avg: 12.808823529411764, max: 41)\n",
      "\n",
      "Run: 69, exploration: 0.018686420266267767, score: 10\n",
      "Scores: (min: 8, avg: 12.768115942028986, max: 41)\n",
      "\n",
      "Run: 70, exploration: 0.01786215438933485, score: 10\n",
      "Scores: (min: 8, avg: 12.728571428571428, max: 41)\n",
      "\n",
      "Run: 71, exploration: 0.01724627885940145, score: 8\n",
      "Scores: (min: 8, avg: 12.661971830985916, max: 41)\n",
      "\n",
      "Run: 72, exploration: 0.0165683801277891, score: 9\n",
      "Scores: (min: 8, avg: 12.61111111111111, max: 41)\n",
      "\n",
      "Run: 73, exploration: 0.01567956241402325, score: 12\n",
      "Scores: (min: 8, avg: 12.602739726027398, max: 41)\n",
      "\n",
      "Run: 74, exploration: 0.01506324653745902, score: 9\n",
      "Scores: (min: 8, avg: 12.554054054054054, max: 41)\n",
      "\n",
      "Run: 75, exploration: 0.014398800381387675, score: 10\n",
      "Scores: (min: 8, avg: 12.52, max: 41)\n",
      "\n",
      "Run: 76, exploration: 0.013289101019874787, score: 17\n",
      "Scores: (min: 8, avg: 12.578947368421053, max: 41)\n",
      "\n",
      "Run: 77, exploration: 0.012576201611761997, score: 12\n",
      "Scores: (min: 8, avg: 12.571428571428571, max: 41)\n",
      "\n",
      "Run: 78, exploration: 0.011961352755726806, score: 11\n",
      "Scores: (min: 8, avg: 12.551282051282051, max: 41)\n",
      "\n",
      "Run: 79, exploration: 0.01, score: 45\n",
      "Scores: (min: 8, avg: 12.962025316455696, max: 45)\n",
      "\n",
      "Run: 80, exploration: 0.01, score: 22\n",
      "Scores: (min: 8, avg: 13.075, max: 45)\n",
      "\n",
      "Run: 81, exploration: 0.01, score: 27\n",
      "Scores: (min: 8, avg: 13.246913580246913, max: 45)\n",
      "\n",
      "Run: 82, exploration: 0.01, score: 61\n",
      "Scores: (min: 8, avg: 13.829268292682928, max: 61)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 83, exploration: 0.01, score: 36\n",
      "Scores: (min: 8, avg: 14.096385542168674, max: 61)\n",
      "\n",
      "Run: 84, exploration: 0.01, score: 53\n",
      "Scores: (min: 8, avg: 14.55952380952381, max: 61)\n",
      "\n",
      "Run: 85, exploration: 0.01, score: 43\n",
      "Scores: (min: 8, avg: 14.894117647058824, max: 61)\n",
      "\n",
      "Run: 86, exploration: 0.01, score: 21\n",
      "Scores: (min: 8, avg: 14.965116279069768, max: 61)\n",
      "\n",
      "Run: 87, exploration: 0.01, score: 84\n",
      "Scores: (min: 8, avg: 15.758620689655173, max: 84)\n",
      "\n",
      "Run: 88, exploration: 0.01, score: 22\n",
      "Scores: (min: 8, avg: 15.829545454545455, max: 84)\n",
      "\n",
      "Run: 89, exploration: 0.01, score: 58\n",
      "Scores: (min: 8, avg: 16.303370786516854, max: 84)\n",
      "\n",
      "Run: 90, exploration: 0.01, score: 39\n",
      "Scores: (min: 8, avg: 16.555555555555557, max: 84)\n",
      "\n",
      "Run: 91, exploration: 0.01, score: 64\n",
      "Scores: (min: 8, avg: 17.076923076923077, max: 84)\n",
      "\n",
      "Run: 92, exploration: 0.01, score: 20\n",
      "Scores: (min: 8, avg: 17.108695652173914, max: 84)\n",
      "\n",
      "Run: 93, exploration: 0.01, score: 18\n",
      "Scores: (min: 8, avg: 17.118279569892472, max: 84)\n",
      "\n",
      "Run: 94, exploration: 0.01, score: 40\n",
      "Scores: (min: 8, avg: 17.361702127659573, max: 84)\n",
      "\n",
      "Run: 95, exploration: 0.01, score: 48\n",
      "Scores: (min: 8, avg: 17.68421052631579, max: 84)\n",
      "\n",
      "Run: 96, exploration: 0.01, score: 80\n",
      "Scores: (min: 8, avg: 18.333333333333332, max: 84)\n",
      "\n",
      "Run: 97, exploration: 0.01, score: 49\n",
      "Scores: (min: 8, avg: 18.649484536082475, max: 84)\n",
      "\n",
      "Run: 98, exploration: 0.01, score: 42\n",
      "Scores: (min: 8, avg: 18.887755102040817, max: 84)\n",
      "\n",
      "Run: 99, exploration: 0.01, score: 46\n",
      "Scores: (min: 8, avg: 19.161616161616163, max: 84)\n",
      "\n",
      "Run: 100, exploration: 0.01, score: 49\n",
      "Scores: (min: 8, avg: 19.46, max: 84)\n",
      "\n",
      "Run: 101, exploration: 0.01, score: 43\n",
      "Scores: (min: 8, avg: 19.78, max: 84)\n",
      "\n",
      "Run: 102, exploration: 0.01, score: 46\n",
      "Scores: (min: 8, avg: 19.92, max: 84)\n",
      "\n",
      "Run: 103, exploration: 0.01, score: 73\n",
      "Scores: (min: 8, avg: 20.41, max: 84)\n",
      "\n",
      "Run: 104, exploration: 0.01, score: 64\n",
      "Scores: (min: 8, avg: 20.64, max: 84)\n",
      "\n",
      "Run: 105, exploration: 0.01, score: 140\n",
      "Scores: (min: 8, avg: 21.89, max: 140)\n",
      "\n",
      "Run: 106, exploration: 0.01, score: 50\n",
      "Scores: (min: 8, avg: 22.23, max: 140)\n",
      "\n",
      "Run: 107, exploration: 0.01, score: 130\n",
      "Scores: (min: 8, avg: 23.13, max: 140)\n",
      "\n",
      "Run: 108, exploration: 0.01, score: 65\n",
      "Scores: (min: 8, avg: 23.61, max: 140)\n",
      "\n",
      "Run: 109, exploration: 0.01, score: 82\n",
      "Scores: (min: 8, avg: 24.33, max: 140)\n",
      "\n",
      "Run: 110, exploration: 0.01, score: 50\n",
      "Scores: (min: 8, avg: 24.71, max: 140)\n",
      "\n",
      "Run: 111, exploration: 0.01, score: 68\n",
      "Scores: (min: 8, avg: 25.31, max: 140)\n",
      "\n",
      "Run: 112, exploration: 0.01, score: 91\n",
      "Scores: (min: 8, avg: 26.12, max: 140)\n",
      "\n",
      "Run: 113, exploration: 0.01, score: 69\n",
      "Scores: (min: 8, avg: 26.71, max: 140)\n",
      "\n",
      "Run: 114, exploration: 0.01, score: 54\n",
      "Scores: (min: 8, avg: 27.17, max: 140)\n",
      "\n",
      "Run: 115, exploration: 0.01, score: 121\n",
      "Scores: (min: 8, avg: 28.25, max: 140)\n",
      "\n",
      "Run: 116, exploration: 0.01, score: 144\n",
      "Scores: (min: 8, avg: 29.6, max: 144)\n",
      "\n",
      "Run: 117, exploration: 0.01, score: 149\n",
      "Scores: (min: 8, avg: 30.98, max: 149)\n",
      "\n",
      "Run: 118, exploration: 0.01, score: 101\n",
      "Scores: (min: 8, avg: 31.9, max: 149)\n",
      "\n",
      "Run: 119, exploration: 0.01, score: 70\n",
      "Scores: (min: 8, avg: 32.49, max: 149)\n",
      "\n",
      "Run: 120, exploration: 0.01, score: 72\n",
      "Scores: (min: 8, avg: 33.09, max: 149)\n",
      "\n",
      "Run: 121, exploration: 0.01, score: 87\n",
      "Scores: (min: 8, avg: 33.86, max: 149)\n",
      "\n",
      "Run: 122, exploration: 0.01, score: 92\n",
      "Scores: (min: 8, avg: 34.67, max: 149)\n",
      "\n",
      "Run: 123, exploration: 0.01, score: 83\n",
      "Scores: (min: 8, avg: 35.38, max: 149)\n",
      "\n",
      "Run: 124, exploration: 0.01, score: 101\n",
      "Scores: (min: 8, avg: 36.25, max: 149)\n",
      "\n",
      "Run: 125, exploration: 0.01, score: 103\n",
      "Scores: (min: 8, avg: 37.2, max: 149)\n",
      "\n",
      "Run: 126, exploration: 0.01, score: 188\n",
      "Scores: (min: 8, avg: 38.98, max: 188)\n",
      "\n",
      "Run: 127, exploration: 0.01, score: 135\n",
      "Scores: (min: 8, avg: 40.24, max: 188)\n",
      "\n",
      "Run: 128, exploration: 0.01, score: 129\n",
      "Scores: (min: 8, avg: 41.39, max: 188)\n",
      "\n",
      "Run: 129, exploration: 0.01, score: 83\n",
      "Scores: (min: 8, avg: 42.12, max: 188)\n",
      "\n",
      "Run: 130, exploration: 0.01, score: 153\n",
      "Scores: (min: 8, avg: 43.56, max: 188)\n",
      "\n",
      "Run: 131, exploration: 0.01, score: 96\n",
      "Scores: (min: 8, avg: 44.43, max: 188)\n",
      "\n",
      "Run: 132, exploration: 0.01, score: 166\n",
      "Scores: (min: 8, avg: 45.97, max: 188)\n",
      "\n",
      "Run: 133, exploration: 0.01, score: 169\n",
      "Scores: (min: 8, avg: 47.55, max: 188)\n",
      "\n",
      "Run: 134, exploration: 0.01, score: 90\n",
      "Scores: (min: 8, avg: 48.35, max: 188)\n",
      "\n",
      "Run: 135, exploration: 0.01, score: 101\n",
      "Scores: (min: 8, avg: 49.27, max: 188)\n",
      "\n",
      "Run: 136, exploration: 0.01, score: 383\n",
      "Scores: (min: 8, avg: 53, max: 383)\n",
      "\n",
      "Run: 137, exploration: 0.01, score: 122\n",
      "Scores: (min: 8, avg: 54.11, max: 383)\n",
      "\n",
      "Run: 138, exploration: 0.01, score: 126\n",
      "Scores: (min: 8, avg: 55.23, max: 383)\n",
      "\n",
      "Run: 139, exploration: 0.01, score: 139\n",
      "Scores: (min: 8, avg: 56.53, max: 383)\n",
      "\n",
      "Run: 140, exploration: 0.01, score: 143\n",
      "Scores: (min: 8, avg: 57.85, max: 383)\n",
      "\n",
      "Run: 141, exploration: 0.01, score: 128\n",
      "Scores: (min: 8, avg: 59.05, max: 383)\n",
      "\n",
      "Run: 142, exploration: 0.01, score: 163\n",
      "Scores: (min: 8, avg: 60.58, max: 383)\n",
      "\n",
      "Run: 143, exploration: 0.01, score: 115\n",
      "Scores: (min: 8, avg: 61.6, max: 383)\n",
      "\n",
      "Run: 144, exploration: 0.01, score: 164\n",
      "Scores: (min: 8, avg: 63.14, max: 383)\n",
      "\n",
      "Run: 145, exploration: 0.01, score: 130\n",
      "Scores: (min: 8, avg: 64.29, max: 383)\n",
      "\n",
      "Run: 146, exploration: 0.01, score: 218\n",
      "Scores: (min: 8, avg: 66.35, max: 383)\n",
      "\n",
      "Run: 147, exploration: 0.01, score: 169\n",
      "Scores: (min: 8, avg: 67.93, max: 383)\n",
      "\n",
      "Run: 148, exploration: 0.01, score: 314\n",
      "Scores: (min: 8, avg: 70.9, max: 383)\n",
      "\n",
      "Run: 149, exploration: 0.01, score: 261\n",
      "Scores: (min: 8, avg: 73.41, max: 383)\n",
      "\n",
      "Run: 150, exploration: 0.01, score: 183\n",
      "Scores: (min: 8, avg: 75.12, max: 383)\n",
      "\n",
      "Run: 151, exploration: 0.01, score: 253\n",
      "Scores: (min: 8, avg: 77.49, max: 383)\n",
      "\n",
      "Run: 152, exploration: 0.01, score: 119\n",
      "Scores: (min: 8, avg: 78.57, max: 383)\n",
      "\n",
      "Run: 153, exploration: 0.01, score: 187\n",
      "Scores: (min: 8, avg: 80.35, max: 383)\n",
      "\n",
      "Run: 154, exploration: 0.01, score: 161\n",
      "Scores: (min: 8, avg: 81.87, max: 383)\n",
      "\n",
      "Run: 155, exploration: 0.01, score: 184\n",
      "Scores: (min: 8, avg: 83.62, max: 383)\n",
      "\n",
      "Run: 156, exploration: 0.01, score: 156\n",
      "Scores: (min: 8, avg: 85.09, max: 383)\n",
      "\n",
      "Run: 157, exploration: 0.01, score: 157\n",
      "Scores: (min: 8, avg: 86.46, max: 383)\n",
      "\n",
      "Run: 158, exploration: 0.01, score: 293\n",
      "Scores: (min: 8, avg: 89.25, max: 383)\n",
      "\n",
      "Run: 159, exploration: 0.01, score: 259\n",
      "Scores: (min: 8, avg: 91.67, max: 383)\n",
      "\n",
      "Run: 160, exploration: 0.01, score: 286\n",
      "Scores: (min: 8, avg: 94.44, max: 383)\n",
      "\n",
      "Run: 161, exploration: 0.01, score: 241\n",
      "Scores: (min: 8, avg: 96.76, max: 383)\n",
      "\n",
      "Run: 162, exploration: 0.01, score: 306\n",
      "Scores: (min: 8, avg: 99.73, max: 383)\n",
      "\n",
      "Run: 163, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 104.63, max: 500)\n",
      "\n",
      "Run: 164, exploration: 0.01, score: 204\n",
      "Scores: (min: 8, avg: 106.57, max: 500)\n",
      "\n",
      "Run: 165, exploration: 0.01, score: 153\n",
      "Scores: (min: 8, avg: 107.83, max: 500)\n",
      "\n",
      "Run: 166, exploration: 0.01, score: 259\n",
      "Scores: (min: 8, avg: 110.33, max: 500)\n",
      "\n",
      "Run: 167, exploration: 0.01, score: 228\n",
      "Scores: (min: 8, avg: 112.46, max: 500)\n",
      "\n",
      "Run: 168, exploration: 0.01, score: 258\n",
      "Scores: (min: 8, avg: 114.95, max: 500)\n",
      "\n",
      "Run: 169, exploration: 0.01, score: 213\n",
      "Scores: (min: 8, avg: 116.98, max: 500)\n",
      "\n",
      "Run: 170, exploration: 0.01, score: 158\n",
      "Scores: (min: 8, avg: 118.46, max: 500)\n",
      "\n",
      "Run: 171, exploration: 0.01, score: 165\n",
      "Scores: (min: 9, avg: 120.03, max: 500)\n",
      "\n",
      "Run: 172, exploration: 0.01, score: 269\n",
      "Scores: (min: 9, avg: 122.63, max: 500)\n",
      "\n",
      "Run: 173, exploration: 0.01, score: 188\n",
      "Scores: (min: 9, avg: 124.39, max: 500)\n",
      "\n",
      "Run: 174, exploration: 0.01, score: 132\n",
      "Scores: (min: 10, avg: 125.62, max: 500)\n",
      "\n",
      "Run: 175, exploration: 0.01, score: 230\n",
      "Scores: (min: 11, avg: 127.82, max: 500)\n",
      "\n",
      "Run: 176, exploration: 0.01, score: 182\n",
      "Scores: (min: 11, avg: 129.47, max: 500)\n",
      "\n",
      "Run: 177, exploration: 0.01, score: 243\n",
      "Scores: (min: 11, avg: 131.78, max: 500)\n",
      "\n",
      "Run: 178, exploration: 0.01, score: 368\n",
      "Scores: (min: 18, avg: 135.35, max: 500)\n",
      "\n",
      "Run: 179, exploration: 0.01, score: 193\n",
      "Scores: (min: 18, avg: 136.83, max: 500)\n",
      "\n",
      "Run: 180, exploration: 0.01, score: 232\n",
      "Scores: (min: 18, avg: 138.93, max: 500)\n",
      "\n",
      "Run: 181, exploration: 0.01, score: 194\n",
      "Scores: (min: 18, avg: 140.6, max: 500)\n",
      "\n",
      "Run: 182, exploration: 0.01, score: 166\n",
      "Scores: (min: 18, avg: 141.65, max: 500)\n",
      "\n",
      "Run: 183, exploration: 0.01, score: 230\n",
      "Scores: (min: 18, avg: 143.59, max: 500)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 184, exploration: 0.01, score: 160\n",
      "Scores: (min: 18, avg: 144.66, max: 500)\n",
      "\n",
      "Run: 185, exploration: 0.01, score: 196\n",
      "Scores: (min: 18, avg: 146.19, max: 500)\n",
      "\n",
      "Run: 186, exploration: 0.01, score: 267\n",
      "Scores: (min: 18, avg: 148.65, max: 500)\n",
      "\n",
      "Run: 187, exploration: 0.01, score: 255\n",
      "Scores: (min: 18, avg: 150.36, max: 500)\n",
      "\n",
      "Run: 188, exploration: 0.01, score: 228\n",
      "Scores: (min: 18, avg: 152.42, max: 500)\n",
      "\n",
      "Run: 189, exploration: 0.01, score: 155\n",
      "Scores: (min: 18, avg: 153.39, max: 500)\n",
      "\n",
      "Run: 190, exploration: 0.01, score: 220\n",
      "Scores: (min: 18, avg: 155.2, max: 500)\n",
      "\n",
      "Run: 191, exploration: 0.01, score: 311\n",
      "Scores: (min: 18, avg: 157.67, max: 500)\n",
      "\n",
      "Run: 192, exploration: 0.01, score: 289\n",
      "Scores: (min: 18, avg: 160.36, max: 500)\n",
      "\n",
      "Run: 193, exploration: 0.01, score: 220\n",
      "Scores: (min: 40, avg: 162.38, max: 500)\n",
      "\n",
      "Run: 194, exploration: 0.01, score: 179\n",
      "Scores: (min: 42, avg: 163.77, max: 500)\n",
      "\n",
      "Run: 195, exploration: 0.01, score: 288\n",
      "Scores: (min: 42, avg: 166.17, max: 500)\n",
      "\n",
      "Run: 196, exploration: 0.01, score: 199\n",
      "Scores: (min: 42, avg: 167.36, max: 500)\n",
      "\n",
      "Run: 197, exploration: 0.01, score: 217\n",
      "Scores: (min: 42, avg: 169.04, max: 500)\n",
      "\n",
      "Run: 198, exploration: 0.01, score: 197\n",
      "Scores: (min: 43, avg: 170.59, max: 500)\n",
      "\n",
      "Run: 199, exploration: 0.01, score: 241\n",
      "Scores: (min: 43, avg: 172.54, max: 500)\n",
      "\n",
      "Run: 200, exploration: 0.01, score: 231\n",
      "Scores: (min: 43, avg: 174.36, max: 500)\n",
      "\n",
      "Run: 201, exploration: 0.01, score: 242\n",
      "Scores: (min: 46, avg: 176.35, max: 500)\n",
      "\n",
      "Run: 202, exploration: 0.01, score: 224\n",
      "Scores: (min: 50, avg: 178.13, max: 500)\n",
      "\n",
      "Run: 203, exploration: 0.01, score: 187\n",
      "Scores: (min: 50, avg: 179.27, max: 500)\n",
      "\n",
      "Run: 204, exploration: 0.01, score: 259\n",
      "Scores: (min: 50, avg: 181.22, max: 500)\n",
      "\n",
      "Run: 205, exploration: 0.01, score: 248\n",
      "Scores: (min: 50, avg: 182.3, max: 500)\n",
      "\n",
      "Run: 206, exploration: 0.01, score: 259\n",
      "Scores: (min: 50, avg: 184.39, max: 500)\n",
      "\n",
      "Run: 207, exploration: 0.01, score: 281\n",
      "Scores: (min: 50, avg: 185.9, max: 500)\n",
      "\n",
      "Run: 208, exploration: 0.01, score: 233\n",
      "Scores: (min: 50, avg: 187.58, max: 500)\n",
      "\n",
      "Run: 209, exploration: 0.01, score: 232\n",
      "Scores: (min: 50, avg: 189.08, max: 500)\n",
      "\n",
      "Run: 210, exploration: 0.01, score: 221\n",
      "Scores: (min: 54, avg: 190.79, max: 500)\n",
      "\n",
      "Run: 211, exploration: 0.01, score: 217\n",
      "Scores: (min: 54, avg: 192.28, max: 500)\n",
      "\n",
      "Run: 212, exploration: 0.01, score: 209\n",
      "Scores: (min: 54, avg: 193.46, max: 500)\n",
      "\n",
      "Run: 213, exploration: 0.01, score: 267\n",
      "Scores: (min: 54, avg: 195.44, max: 500)\n",
      "\n",
      "Solved in 113 runs, 213 total runs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e23913c48dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-bea6327a4fd8>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Run: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", exploration: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", score: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mscore_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\Cartpole\\scores\\score_logger.py\u001b[0m in \u001b[0;36madd_score\u001b[1;34m(self, score, run)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m#               show_trend=False,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m#               show_legend=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_of_n_last\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_goal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_trend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_legend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the code is running properly, you should begin to see output appearing above this code block. It will take several minutes, so it is recommended that you let this code run in the background while completing other work. When the code has finished, it will print output saying, \"Solved in _ runs, _ total runs.\"\n",
    "\n",
    "You may see an error about not having an exit command. This error does not affect the program's functionality and results from the steps taken to convert the code from Python 2.x to Python 3. Please disregard this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 19\n",
      "Scores: (min: 19, avg: 19, max: 19)\n",
      "\n",
      "Run: 2, exploration: 0.9091562615825302, score: 20\n",
      "Scores: (min: 19, avg: 19.5, max: 20)\n",
      "\n",
      "Run: 3, exploration: 0.6935613678313175, score: 55\n",
      "Scores: (min: 19, avg: 31.333333333333332, max: 55)\n",
      "\n",
      "Run: 4, exploration: 0.6149486215357263, score: 25\n",
      "Scores: (min: 19, avg: 29.75, max: 55)\n",
      "\n",
      "Run: 5, exploration: 0.5878229785513479, score: 10\n",
      "Scores: (min: 10, avg: 25.8, max: 55)\n",
      "\n",
      "Run: 6, exploration: 0.547986285490042, score: 15\n",
      "Scores: (min: 10, avg: 24, max: 55)\n",
      "\n",
      "Run: 7, exploration: 0.5159963842937159, score: 13\n",
      "Scores: (min: 10, avg: 22.428571428571427, max: 55)\n",
      "\n",
      "Run: 8, exploration: 0.42013897252428334, score: 42\n",
      "Scores: (min: 10, avg: 24.875, max: 55)\n",
      "\n",
      "Run: 9, exploration: 0.3858205374665315, score: 18\n",
      "Scores: (min: 10, avg: 24.11111111111111, max: 55)\n",
      "\n",
      "Run: 10, exploration: 0.3507711574848344, score: 20\n",
      "Scores: (min: 10, avg: 23.7, max: 55)\n",
      "\n",
      "Run: 11, exploration: 0.27714603575484437, score: 48\n",
      "Scores: (min: 10, avg: 25.90909090909091, max: 55)\n",
      "\n",
      "Run: 12, exploration: 0.22340924607110255, score: 44\n",
      "Scores: (min: 10, avg: 27.416666666666668, max: 55)\n",
      "\n",
      "Run: 13, exploration: 0.16873052768933355, score: 57\n",
      "Scores: (min: 10, avg: 29.692307692307693, max: 57)\n",
      "\n",
      "Run: 14, exploration: 0.16048131420416054, score: 11\n",
      "Scores: (min: 10, avg: 28.357142857142858, max: 57)\n",
      "\n",
      "Run: 15, exploration: 0.13001512070402377, score: 43\n",
      "Scores: (min: 10, avg: 29.333333333333332, max: 57)\n",
      "\n",
      "Run: 16, exploration: 0.10800599224428936, score: 38\n",
      "Scores: (min: 10, avg: 29.875, max: 57)\n",
      "\n",
      "Run: 17, exploration: 0.07196434741762824, score: 82\n",
      "Scores: (min: 10, avg: 32.94117647058823, max: 82)\n",
      "\n",
      "Run: 18, exploration: 0.06542683706543727, score: 20\n",
      "Scores: (min: 10, avg: 32.22222222222222, max: 82)\n",
      "\n",
      "Run: 19, exploration: 0.062228127537270826, score: 11\n",
      "Scores: (min: 10, avg: 31.105263157894736, max: 82)\n",
      "\n",
      "Run: 20, exploration: 0.05462441922520914, score: 27\n",
      "Scores: (min: 10, avg: 30.9, max: 82)\n",
      "\n",
      "Run: 21, exploration: 0.04104898613852031, score: 58\n",
      "Scores: (min: 10, avg: 32.19047619047619, max: 82)\n",
      "\n",
      "Run: 22, exploration: 0.03826710124979409, score: 15\n",
      "Scores: (min: 10, avg: 31.40909090909091, max: 82)\n",
      "\n",
      "Run: 23, exploration: 0.03227084302572862, score: 35\n",
      "Scores: (min: 10, avg: 31.565217391304348, max: 82)\n",
      "\n",
      "Run: 24, exploration: 0.022269900979844076, score: 75\n",
      "Scores: (min: 10, avg: 33.375, max: 82)\n",
      "\n",
      "Run: 25, exploration: 0.018223908064988973, score: 41\n",
      "Scores: (min: 10, avg: 33.68, max: 82)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reduce GAMMA\n",
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.55  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:\n",
    "            if step > 100:\n",
    "                return\n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n",
    "\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 13\n",
      "Scores: (min: 13, avg: 13, max: 13)\n",
      "\n",
      "Run: 2, exploration: 0.9558895783575597, score: 16\n",
      "Scores: (min: 13, avg: 14.5, max: 16)\n",
      "\n",
      "Run: 3, exploration: 0.8866535105013078, score: 16\n",
      "Scores: (min: 13, avg: 15, max: 16)\n",
      "\n",
      "Run: 4, exploration: 0.736559652908221, score: 38\n",
      "Scores: (min: 13, avg: 20.75, max: 38)\n",
      "\n",
      "Run: 5, exploration: 0.6465587967553006, score: 27\n",
      "Scores: (min: 13, avg: 22, max: 38)\n",
      "\n",
      "Run: 6, exploration: 0.6088145090359074, score: 13\n",
      "Scores: (min: 13, avg: 20.5, max: 38)\n",
      "\n",
      "Run: 7, exploration: 0.567555222460375, score: 15\n",
      "Scores: (min: 13, avg: 19.714285714285715, max: 38)\n",
      "\n",
      "Run: 8, exploration: 0.5371084840724134, score: 12\n",
      "Scores: (min: 12, avg: 18.75, max: 38)\n",
      "\n",
      "Run: 9, exploration: 0.5185893309484582, score: 8\n",
      "Scores: (min: 8, avg: 17.555555555555557, max: 38)\n",
      "\n",
      "Run: 10, exploration: 0.46211964903917074, score: 24\n",
      "Scores: (min: 8, avg: 18.2, max: 38)\n",
      "\n",
      "Run: 11, exploration: 0.4417353564707963, score: 10\n",
      "Scores: (min: 8, avg: 17.454545454545453, max: 38)\n",
      "\n",
      "Run: 12, exploration: 0.4159480862733536, score: 13\n",
      "Scores: (min: 8, avg: 17.083333333333332, max: 38)\n",
      "\n",
      "Run: 13, exploration: 0.39561243860243744, score: 11\n",
      "Scores: (min: 8, avg: 16.615384615384617, max: 38)\n",
      "\n",
      "Run: 14, exploration: 0.37816180712868996, score: 10\n",
      "Scores: (min: 8, avg: 16.142857142857142, max: 38)\n",
      "\n",
      "Run: 15, exploration: 0.3596735257153405, score: 11\n",
      "Scores: (min: 8, avg: 15.8, max: 38)\n",
      "\n",
      "Run: 16, exploration: 0.3420891339682016, score: 11\n",
      "Scores: (min: 8, avg: 15.5, max: 38)\n",
      "\n",
      "Run: 17, exploration: 0.32864265128599696, score: 9\n",
      "Scores: (min: 8, avg: 15.117647058823529, max: 38)\n",
      "\n",
      "Run: 18, exploration: 0.3125753549412418, score: 11\n",
      "Scores: (min: 8, avg: 14.88888888888889, max: 38)\n",
      "\n",
      "Run: 19, exploration: 0.2987875242397482, score: 10\n",
      "Scores: (min: 8, avg: 14.631578947368421, max: 38)\n",
      "\n",
      "Run: 20, exploration: 0.285607880564032, score: 10\n",
      "Scores: (min: 8, avg: 14.4, max: 38)\n",
      "\n",
      "Run: 21, exploration: 0.27164454854530906, score: 11\n",
      "Scores: (min: 8, avg: 14.238095238095237, max: 38)\n",
      "\n",
      "Run: 22, exploration: 0.2583638820072446, score: 11\n",
      "Scores: (min: 8, avg: 14.090909090909092, max: 38)\n",
      "\n",
      "Run: 23, exploration: 0.2408545925762412, score: 15\n",
      "Scores: (min: 8, avg: 14.130434782608695, max: 38)\n",
      "\n",
      "Run: 24, exploration: 0.23255008201124722, score: 8\n",
      "Scores: (min: 8, avg: 13.875, max: 38)\n",
      "\n",
      "Run: 25, exploration: 0.22229219984074702, score: 10\n",
      "Scores: (min: 8, avg: 13.72, max: 38)\n",
      "\n",
      "Run: 26, exploration: 0.21248679717794605, score: 10\n",
      "Scores: (min: 8, avg: 13.576923076923077, max: 38)\n",
      "\n",
      "Run: 27, exploration: 0.2041345879004775, score: 9\n",
      "Scores: (min: 8, avg: 13.407407407407407, max: 38)\n",
      "\n",
      "Run: 28, exploration: 0.19611067854912728, score: 9\n",
      "Scores: (min: 8, avg: 13.25, max: 38)\n",
      "\n",
      "Run: 29, exploration: 0.18559023879528855, score: 12\n",
      "Scores: (min: 8, avg: 13.206896551724139, max: 38)\n",
      "\n",
      "Run: 30, exploration: 0.1756341724525918, score: 12\n",
      "Scores: (min: 8, avg: 13.166666666666666, max: 38)\n",
      "\n",
      "Run: 31, exploration: 0.16873052768933355, score: 9\n",
      "Scores: (min: 8, avg: 13.03225806451613, max: 38)\n",
      "\n",
      "Run: 32, exploration: 0.16128775296900558, score: 10\n",
      "Scores: (min: 8, avg: 12.9375, max: 38)\n",
      "\n",
      "Run: 33, exploration: 0.15417328217978102, score: 10\n",
      "Scores: (min: 8, avg: 12.848484848484848, max: 38)\n",
      "\n",
      "Run: 34, exploration: 0.1473726336968319, score: 10\n",
      "Scores: (min: 8, avg: 12.764705882352942, max: 38)\n",
      "\n",
      "Run: 35, exploration: 0.14157986400593744, score: 9\n",
      "Scores: (min: 8, avg: 12.657142857142857, max: 38)\n",
      "\n",
      "Run: 36, exploration: 0.13398475271138335, score: 12\n",
      "Scores: (min: 8, avg: 12.63888888888889, max: 38)\n",
      "\n",
      "Run: 37, exploration: 0.12936504510050365, score: 8\n",
      "Scores: (min: 8, avg: 12.513513513513514, max: 38)\n",
      "\n",
      "Run: 38, exploration: 0.12490462201997637, score: 8\n",
      "Scores: (min: 8, avg: 12.394736842105264, max: 38)\n",
      "\n",
      "Run: 39, exploration: 0.11820406108847166, score: 12\n",
      "Scores: (min: 8, avg: 12.384615384615385, max: 38)\n",
      "\n",
      "Run: 40, exploration: 0.11019338598389174, score: 15\n",
      "Scores: (min: 8, avg: 12.45, max: 38)\n",
      "\n",
      "Run: 41, exploration: 0.1063939893092943, score: 8\n",
      "Scores: (min: 8, avg: 12.341463414634147, max: 38)\n",
      "\n",
      "Run: 42, exploration: 0.10272559337455119, score: 8\n",
      "Scores: (min: 8, avg: 12.238095238095237, max: 38)\n",
      "\n",
      "Run: 43, exploration: 0.09918368135888474, score: 8\n",
      "Scores: (min: 8, avg: 12.13953488372093, max: 38)\n",
      "\n",
      "Run: 44, exploration: 0.09480864735409487, score: 10\n",
      "Scores: (min: 8, avg: 12.090909090909092, max: 38)\n",
      "\n",
      "Run: 45, exploration: 0.09108200798387568, score: 9\n",
      "Scores: (min: 8, avg: 12.022222222222222, max: 38)\n",
      "\n",
      "Run: 46, exploration: 0.08662902049662846, score: 11\n",
      "Scores: (min: 8, avg: 12, max: 38)\n",
      "\n",
      "Run: 47, exploration: 0.08280777787605056, score: 10\n",
      "Scores: (min: 8, avg: 11.957446808510639, max: 38)\n",
      "\n",
      "Run: 48, exploration: 0.07955285615946175, score: 9\n",
      "Scores: (min: 8, avg: 11.895833333333334, max: 38)\n",
      "\n",
      "Run: 49, exploration: 0.07053581918419231, score: 25\n",
      "Scores: (min: 8, avg: 12.16326530612245, max: 38)\n",
      "\n",
      "Run: 50, exploration: 0.06742445445908266, score: 10\n",
      "Scores: (min: 8, avg: 12.12, max: 38)\n",
      "\n",
      "Run: 51, exploration: 0.06445033334388098, score: 10\n",
      "Scores: (min: 8, avg: 12.07843137254902, max: 38)\n",
      "\n",
      "Run: 52, exploration: 0.062228127537270826, score: 8\n",
      "Scores: (min: 8, avg: 12, max: 38)\n",
      "\n",
      "Run: 53, exploration: 0.05918580250061433, score: 11\n",
      "Scores: (min: 8, avg: 11.981132075471699, max: 38)\n",
      "\n",
      "Run: 54, exploration: 0.05685938874076987, score: 9\n",
      "Scores: (min: 8, avg: 11.925925925925926, max: 38)\n",
      "\n",
      "Run: 55, exploration: 0.05041457663196215, score: 25\n",
      "Scores: (min: 8, avg: 12.163636363636364, max: 38)\n",
      "\n",
      "Run: 56, exploration: 0.04676299685992172, score: 16\n",
      "Scores: (min: 8, avg: 12.232142857142858, max: 38)\n",
      "\n",
      "Run: 57, exploration: 0.04230229704853423, score: 21\n",
      "Scores: (min: 8, avg: 12.385964912280702, max: 38)\n",
      "\n",
      "Run: 58, exploration: 0.03884689602888724, score: 18\n",
      "Scores: (min: 8, avg: 12.482758620689655, max: 38)\n",
      "\n",
      "Run: 59, exploration: 0.03585300941485119, score: 17\n",
      "Scores: (min: 8, avg: 12.559322033898304, max: 38)\n",
      "\n",
      "Run: 60, exploration: 0.032433008066058915, score: 21\n",
      "Scores: (min: 8, avg: 12.7, max: 38)\n",
      "\n",
      "Run: 61, exploration: 0.029339239003388088, score: 21\n",
      "Scores: (min: 8, avg: 12.836065573770492, max: 38)\n",
      "\n",
      "Run: 62, exploration: 0.026807992814067, score: 19\n",
      "Scores: (min: 8, avg: 12.935483870967742, max: 38)\n",
      "\n",
      "Run: 63, exploration: 0.023769400938086327, score: 25\n",
      "Scores: (min: 8, avg: 13.126984126984127, max: 38)\n",
      "\n",
      "Run: 64, exploration: 0.021075222784267326, score: 25\n",
      "Scores: (min: 8, avg: 13.3125, max: 38)\n",
      "\n",
      "Run: 65, exploration: 0.018969543067746772, score: 22\n",
      "Scores: (min: 8, avg: 13.446153846153846, max: 38)\n",
      "\n",
      "Run: 66, exploration: 0.017595559502304726, score: 16\n",
      "Scores: (min: 8, avg: 13.484848484848484, max: 38)\n",
      "\n",
      "Run: 67, exploration: 0.015997113097568336, score: 20\n",
      "Scores: (min: 8, avg: 13.582089552238806, max: 38)\n",
      "\n",
      "Run: 68, exploration: 0.013972200057807112, score: 28\n",
      "Scores: (min: 8, avg: 13.794117647058824, max: 38)\n",
      "\n",
      "Run: 69, exploration: 0.011723913930325095, score: 36\n",
      "Scores: (min: 8, avg: 14.115942028985508, max: 38)\n",
      "\n",
      "Run: 70, exploration: 0.010087070213020206, score: 31\n",
      "Scores: (min: 8, avg: 14.357142857142858, max: 38)\n",
      "\n",
      "Run: 71, exploration: 0.01, score: 26\n",
      "Scores: (min: 8, avg: 14.52112676056338, max: 38)\n",
      "\n",
      "Run: 72, exploration: 0.01, score: 19\n",
      "Scores: (min: 8, avg: 14.583333333333334, max: 38)\n",
      "\n",
      "Run: 73, exploration: 0.01, score: 18\n",
      "Scores: (min: 8, avg: 14.63013698630137, max: 38)\n",
      "\n",
      "Run: 74, exploration: 0.01, score: 25\n",
      "Scores: (min: 8, avg: 14.77027027027027, max: 38)\n",
      "\n",
      "Run: 75, exploration: 0.01, score: 21\n",
      "Scores: (min: 8, avg: 14.853333333333333, max: 38)\n",
      "\n",
      "Run: 76, exploration: 0.01, score: 25\n",
      "Scores: (min: 8, avg: 14.986842105263158, max: 38)\n",
      "\n",
      "Run: 77, exploration: 0.01, score: 35\n",
      "Scores: (min: 8, avg: 15.246753246753247, max: 38)\n",
      "\n",
      "Run: 78, exploration: 0.01, score: 31\n",
      "Scores: (min: 8, avg: 15.448717948717949, max: 38)\n",
      "\n",
      "Run: 79, exploration: 0.01, score: 30\n",
      "Scores: (min: 8, avg: 15.632911392405063, max: 38)\n",
      "\n",
      "Run: 80, exploration: 0.01, score: 21\n",
      "Scores: (min: 8, avg: 15.7, max: 38)\n",
      "\n",
      "Run: 81, exploration: 0.01, score: 29\n",
      "Scores: (min: 8, avg: 15.864197530864198, max: 38)\n",
      "\n",
      "Run: 82, exploration: 0.01, score: 34\n",
      "Scores: (min: 8, avg: 16.085365853658537, max: 38)\n",
      "\n",
      "Run: 83, exploration: 0.01, score: 43\n",
      "Scores: (min: 8, avg: 16.40963855421687, max: 43)\n",
      "\n",
      "Run: 84, exploration: 0.01, score: 20\n",
      "Scores: (min: 8, avg: 16.452380952380953, max: 43)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 85, exploration: 0.01, score: 20\n",
      "Scores: (min: 8, avg: 16.49411764705882, max: 43)\n",
      "\n",
      "Run: 86, exploration: 0.01, score: 32\n",
      "Scores: (min: 8, avg: 16.674418604651162, max: 43)\n",
      "\n",
      "Run: 87, exploration: 0.01, score: 27\n",
      "Scores: (min: 8, avg: 16.79310344827586, max: 43)\n",
      "\n",
      "Run: 88, exploration: 0.01, score: 51\n",
      "Scores: (min: 8, avg: 17.181818181818183, max: 51)\n",
      "\n",
      "Run: 89, exploration: 0.01, score: 55\n",
      "Scores: (min: 8, avg: 17.60674157303371, max: 55)\n",
      "\n",
      "Run: 90, exploration: 0.01, score: 32\n",
      "Scores: (min: 8, avg: 17.766666666666666, max: 55)\n",
      "\n",
      "Run: 91, exploration: 0.01, score: 28\n",
      "Scores: (min: 8, avg: 17.87912087912088, max: 55)\n",
      "\n",
      "Run: 92, exploration: 0.01, score: 24\n",
      "Scores: (min: 8, avg: 17.945652173913043, max: 55)\n",
      "\n",
      "Run: 93, exploration: 0.01, score: 31\n",
      "Scores: (min: 8, avg: 18.086021505376344, max: 55)\n",
      "\n",
      "Run: 94, exploration: 0.01, score: 57\n",
      "Scores: (min: 8, avg: 18.5, max: 57)\n",
      "\n",
      "Run: 95, exploration: 0.01, score: 34\n",
      "Scores: (min: 8, avg: 18.66315789473684, max: 57)\n",
      "\n",
      "Run: 96, exploration: 0.01, score: 24\n",
      "Scores: (min: 8, avg: 18.71875, max: 57)\n",
      "\n",
      "Run: 97, exploration: 0.01, score: 29\n",
      "Scores: (min: 8, avg: 18.824742268041238, max: 57)\n",
      "\n",
      "Run: 98, exploration: 0.01, score: 27\n",
      "Scores: (min: 8, avg: 18.908163265306122, max: 57)\n",
      "\n",
      "Run: 99, exploration: 0.01, score: 74\n",
      "Scores: (min: 8, avg: 19.464646464646464, max: 74)\n",
      "\n",
      "Run: 100, exploration: 0.01, score: 46\n",
      "Scores: (min: 8, avg: 19.73, max: 74)\n",
      "\n",
      "Run: 101, exploration: 0.01, score: 46\n",
      "Scores: (min: 8, avg: 20.06, max: 74)\n",
      "\n",
      "Run: 102, exploration: 0.01, score: 96\n",
      "Scores: (min: 8, avg: 20.86, max: 96)\n",
      "\n",
      "Run: 103, exploration: 0.01, score: 63\n",
      "Scores: (min: 8, avg: 21.33, max: 96)\n",
      "\n",
      "Run: 104, exploration: 0.01, score: 57\n",
      "Scores: (min: 8, avg: 21.52, max: 96)\n",
      "\n",
      "Run: 105, exploration: 0.01, score: 63\n",
      "Scores: (min: 8, avg: 21.88, max: 96)\n",
      "\n",
      "Run: 106, exploration: 0.01, score: 65\n",
      "Scores: (min: 8, avg: 22.4, max: 96)\n",
      "\n",
      "Run: 107, exploration: 0.01, score: 53\n",
      "Scores: (min: 8, avg: 22.78, max: 96)\n",
      "\n",
      "Run: 108, exploration: 0.01, score: 16\n",
      "Scores: (min: 8, avg: 22.82, max: 96)\n",
      "\n",
      "Run: 109, exploration: 0.01, score: 13\n",
      "Scores: (min: 8, avg: 22.87, max: 96)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10x Learning Rate\n",
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.0001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:\n",
    "            if step > 100:\n",
    "                return\n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n",
    "\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 18\n",
      "Scores: (min: 18, avg: 18, max: 18)\n",
      "\n",
      "Run: 2, exploration: 0.01, score: 13\n",
      "Scores: (min: 13, avg: 15.5, max: 18)\n",
      "\n",
      "Run: 3, exploration: 0.01, score: 10\n",
      "Scores: (min: 10, avg: 13.666666666666666, max: 18)\n",
      "\n",
      "Run: 4, exploration: 0.01, score: 9\n",
      "Scores: (min: 9, avg: 12.5, max: 18)\n",
      "\n",
      "Run: 5, exploration: 0.01, score: 9\n",
      "Scores: (min: 9, avg: 11.8, max: 18)\n",
      "\n",
      "Run: 6, exploration: 0.01, score: 10\n",
      "Scores: (min: 9, avg: 11.5, max: 18)\n",
      "\n",
      "Run: 7, exploration: 0.01, score: 10\n",
      "Scores: (min: 9, avg: 11.285714285714286, max: 18)\n",
      "\n",
      "Run: 8, exploration: 0.01, score: 10\n",
      "Scores: (min: 9, avg: 11.125, max: 18)\n",
      "\n",
      "Run: 9, exploration: 0.01, score: 9\n",
      "Scores: (min: 9, avg: 10.88888888888889, max: 18)\n",
      "\n",
      "Run: 10, exploration: 0.01, score: 10\n",
      "Scores: (min: 9, avg: 10.8, max: 18)\n",
      "\n",
      "Run: 11, exploration: 0.01, score: 8\n",
      "Scores: (min: 8, avg: 10.545454545454545, max: 18)\n",
      "\n",
      "Run: 12, exploration: 0.01, score: 9\n",
      "Scores: (min: 8, avg: 10.416666666666666, max: 18)\n",
      "\n",
      "Run: 13, exploration: 0.01, score: 9\n",
      "Scores: (min: 8, avg: 10.307692307692308, max: 18)\n",
      "\n",
      "Run: 14, exploration: 0.01, score: 8\n",
      "Scores: (min: 8, avg: 10.142857142857142, max: 18)\n",
      "\n",
      "Run: 15, exploration: 0.01, score: 9\n",
      "Scores: (min: 8, avg: 10.066666666666666, max: 18)\n",
      "\n",
      "Run: 16, exploration: 0.01, score: 14\n",
      "Scores: (min: 8, avg: 10.3125, max: 18)\n",
      "\n",
      "Run: 17, exploration: 0.01, score: 16\n",
      "Scores: (min: 8, avg: 10.647058823529411, max: 18)\n",
      "\n",
      "Run: 18, exploration: 0.01, score: 11\n",
      "Scores: (min: 8, avg: 10.666666666666666, max: 18)\n",
      "\n",
      "Run: 19, exploration: 0.01, score: 11\n",
      "Scores: (min: 8, avg: 10.68421052631579, max: 18)\n",
      "\n",
      "Run: 20, exploration: 0.01, score: 18\n",
      "Scores: (min: 8, avg: 11.05, max: 18)\n",
      "\n",
      "Run: 21, exploration: 0.01, score: 18\n",
      "Scores: (min: 8, avg: 11.380952380952381, max: 18)\n",
      "\n",
      "Run: 22, exploration: 0.01, score: 19\n",
      "Scores: (min: 8, avg: 11.727272727272727, max: 19)\n",
      "\n",
      "Run: 23, exploration: 0.01, score: 9\n",
      "Scores: (min: 8, avg: 11.608695652173912, max: 19)\n",
      "\n",
      "Run: 24, exploration: 0.01, score: 12\n",
      "Scores: (min: 8, avg: 11.625, max: 19)\n",
      "\n",
      "Run: 25, exploration: 0.01, score: 17\n",
      "Scores: (min: 8, avg: 11.84, max: 19)\n",
      "\n",
      "Run: 26, exploration: 0.01, score: 22\n",
      "Scores: (min: 8, avg: 12.23076923076923, max: 22)\n",
      "\n",
      "Run: 27, exploration: 0.01, score: 17\n",
      "Scores: (min: 8, avg: 12.407407407407407, max: 22)\n",
      "\n",
      "Run: 28, exploration: 0.01, score: 29\n",
      "Scores: (min: 8, avg: 13, max: 29)\n",
      "\n",
      "Run: 29, exploration: 0.01, score: 31\n",
      "Scores: (min: 8, avg: 13.620689655172415, max: 31)\n",
      "\n",
      "Run: 30, exploration: 0.01, score: 64\n",
      "Scores: (min: 8, avg: 15.3, max: 64)\n",
      "\n",
      "Run: 31, exploration: 0.01, score: 31\n",
      "Scores: (min: 8, avg: 15.806451612903226, max: 64)\n",
      "\n",
      "Run: 32, exploration: 0.01, score: 17\n",
      "Scores: (min: 8, avg: 15.84375, max: 64)\n",
      "\n",
      "Run: 33, exploration: 0.01, score: 35\n",
      "Scores: (min: 8, avg: 16.424242424242426, max: 64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decrease Exploration Decay\n",
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.595  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:\n",
    "            if step > 100:\n",
    "                return\n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n",
    "\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow exploration min and max\n",
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 0.7  \n",
    "EXPLORATION_MIN = 0.4  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:\n",
    "            if step > 100:\n",
    "                return\n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n",
    "\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how reinforcement learning concepts apply to the cartpole problem.\n",
    "    The nature of the cartpole problem is that the bottom of the system (I will call it the \"truck\") nees to be centered under the center of gravity of the pole. If not, the pole begins to fall and the truck needs to center itself again. As a reinforcement learning algorithm, the agent (truck) can receive rewards for keeping the pole upright and punishment for allowing it to lean or fall. The outputs of our system would be either left, right, or stay still.\n",
    "    \n",
    "What is the goal of the agent in this case?\n",
    "    The goal of the agent in the cartpole problem is to keep the pole upright by moving the truck towards the poles center of gravity.\n",
    "What are the various state values?        \n",
    "    0\tCart Position\n",
    "    1\tCart Velocity\n",
    "    2\tPole Angle   \n",
    "    3\tPole Velocity At Tip \n",
    "What are the possible actions that can be performed?\n",
    "    Left, right. There are only 2 options\n",
    "What reinforcement algorithm is used for this problem?\n",
    "    This example uses a Q-Learning algorithm.\n",
    "Analyze how experience replay is applied to the cartpole problem.\n",
    "    Each run of the situation is remembered. Then, when the system tries to determine its next action, it will look back in these memories for a similar situaion and act accordingly similar to biological memory.\n",
    "How does experience replay work in this algorithm?\n",
    "    Each run of the situation is remembered. Then, when the system tries to determine its next action, it will look back in these memories for a similar situaion and act accordingly similar to biological memory.\n",
    "What is the effect of introducing a discount factor for calculating the future rewards?\n",
    "    The discount is multiplied against the estimation of optimal future, so this would mean that we are reducing how much we take the future into account when making our decision.\n",
    "Analyze how neural networks are used in deep Q-learning.\n",
    "    The network is a sequential network that has an input layer and 2 hidden layers. The output layer of the network allows for 2 outputs, left or right. The input is our observation space. The network utalizes both relu and linear activation.\n",
    "Explain the neural network architecture that is used in the cartpole problem.\n",
    "    The network is a sequential network that has an input layer and 2 hidden layers. The output layer of the network allows for 2 outputs, left or right. The input is our observation space. The network utalizes both relu and linear activation.\n",
    "How does the neural network make the Q-learning algorithm more efficient?\n",
    "    By using a neural network it allows the system to avoid programatically analyzing the history and allows it to make an informed decision based on its own memory matrix\n",
    "What difference do you see in the algorithm performance when you increase or decrease the learning rate?\n",
    "    When I increase learing rate (i multiplied by x10) each run was significantly faster, however there was little to no improvement between each run. When learning reate was reduced (again by a factor of 10) the system learned much more each run, and each run took longer to complete.\n",
    "    \n",
    "Citation: \n",
    "Surma, G. (2019, November 10). Cartpole - introduction to reinforcement Learning (DQN - DEEP Q-LEARNING). Retrieved February 07, 2021, from https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
